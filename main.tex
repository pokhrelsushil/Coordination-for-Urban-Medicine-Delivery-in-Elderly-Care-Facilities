\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref} % Include this in the preamble
\geometry{letterpaper, margin=1in}
\usepackage{noto}

\begin{document}

\title{Adaptive Multi-Agent Reinforcement Learning-Coordinated Multirobot Systems for Medication Delivery in Elderly Care Facilities}


\author{
\IEEEauthorblockN{Sushil Pokhrel, PhD Candidate, 21110127}
\IEEEauthorblockA{Dept. of Mechanical Engineering \& Systems Design Engineering\\
University of Waterloo, Ontario, Canada\\
Email: \textbf{\textit{Sushil.Pokhrel@uwaterloo.ca}}}
\title{Adaptive Multi-Agent Coordination for Urban Medicine Delivery in Elderly Care Facilities}
\author{\IEEEauthorblockN{SUSHIL POKHREL, PhD Candidate}
\IEEEauthorblockA{Department of Mechanical engineering/ Systems Design Engineering\\
University of Waterloo, Waterloo, Ontario, Canada\\
Email: sushil.pokhrel@uwaterloo.ca\\
GitHub: \url{https://github.com/pokhrelsushil}}


NB : \textbf{AFTER THE COURSE PROJECT SUBMISSION , WILL BE SUBMITTED TO \textit{\href{https://arxiv.org/login?next_page=/user/}{ARXIV}} FOR PUBLICATION}
}
I have prepared  long version of the print copy for paper publication 
}

\maketitle    


\begin{abstract}\textbf{}
Efficient and reliable medication delivery systems within elderly care facilities are crucial, where patient needs are time-sensitive and influenced by limited staff availability. Multi-robot systems, employing adaptive Multi-Agent Reinforcement Learning (MARL), effectively address these challenges by coordinating Autonomous Mobile Robots (AMRs) for timely deliveries. Such systems, leveraging robust simulations (e.g., Webots), enable navigation of complex environments with static and dynamic obstacles, enhancing operational efficiency in care homes. The implementation of advanced MARL algorithms such as Counterfactual Multi-Agent (COMA) and Factored Centralized Actor-Critic (FACMAC), integrated within the Robot Operating System 2 (ROS 2) framework and the PyTorch library, improves the coordination of multiple robots in real-time scenarios where human interactions vary, allowing for adaptive task allocations. These algorithms enhance robots' ability to predict and respond to the actions of individuals within the care facility. Results from extensive training reveal that MARL-coordinated robots significantly outperform classical multi-agent pathfinding systems, achieving 92--95\% completion rates with fewer collisions and improved routing efficiency. Additionally, insights underscore the importance of utilizing digital twin simulations for safe training environments, highlighting the potential for deploying multiple physical robots. The findings support the growing narrative in the literature advocating for the integration of intelligent robotic systems in eldercare to mitigate caregiver shortages and enhance care quality. In conclusion, leveraging advanced algorithms in robotics lays the groundwork for more effective and efficient medication delivery systems within elderly care settings. Future work should focus on transferring learned policies from simulation to real-world applications while ensuring ethical considerations, particularly regarding the relationship dynamics between robots and elderly patients.
\end{abstract}

 \begin{IEEEkeywords}
\textit{Multi-Agent Reinforcement Learning, Autonomous Mobile Robots, Medication Delivery, Elderly Care, Webots Simulation, ROS 2, Task Allocation, Human-Robot Interaction}
\end{IEEEkeywords}





\section{Introduction}
The increasing interest in service robots for assisting with daily caregiving duties in assisted living and nursing homes is primarily driven by the challenges posed by an aging population. One significant task that stands to benefit from robotic assistance is medication delivery, a labor-intensive duty typically managed by nursing staff \citep{bao2021}. Autonomous medication delivery robots can enable caregivers to focus more on direct patient interactions, thereby improving the quality of care provided \citep{jeon2016}.

    \label{fig:placeholder}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{In-Home Care for Older Adults.png}
    \caption{Different types of In-Home Care  Older Adults Robots}
    \label{
\section{\textbf{In-Home Care for Older Adults Robots}}

 }
\end{figure}
Nonetheless, deploying multiple robots in a dynamic environment such as an assisted living facility poses considerable logistical challenges, primarily due to unpredictable resident and staff movement throughout common areas. Robots must effectively navigate to avoid collisions while delivering medication on schedule \citep{song2020}. Centralized coordination methods, often seen in traditional Multi-Agent Path Finding (MAPF) algorithms, may initially assume collision-free routes by precomputing paths for each robot \citep{wang2023}. However, these approaches are less effective in dynamic environments typical of eldercare facilities, where robots encounter shifting task priorities and unexpected obstacles \citep{sharon2015}.

The challenges of real-time coordination and navigation in the presence of dynamic obstacles call for advanced approaches such as Multi-Agent Reinforcement Learning (MARL). This method allows robots (agents) to learn collaborative navigation strategies through interactive learning experiences, adapting their behavior based on feedback from their environment \citep{liu2023}. MARL has shown promise in other domains such as traffic management and warehouse automation \citep{yu2023}, yet its applicability within the context of eldercare remains largely unexamined, warranting further investigation.

This study explores the application of MARL to develop a robust multi-agent coordination system for medication delivery in assisted living settings. Our contributions are threefold. First, we integrate realistic robotic models alongside simulations of human behavior using advanced MARL algorithms. By employing the COMA and FACMAC techniques within a simulation framework based on ROS 2 and Webots, we aim to enhance cooperative navigation among delivery robots \citep{salinas2023}. Second, we introduce an auction-based task allocation mechanism, empowering robots to bid for delivery tasks based on estimated travel times, ensuring quick and efficient assignment of tasks in real-time \citep{kayy2017}. Finally, we have developed a novel dynamic obstacle model that leverages large language model (LLM) agents to simulate pedestrian movements, thereby improving the robots' situational awareness \citep{rostumi2019}.

The rest of this paper is organized as follows: Section \ref{sec:related} reviews existing research on robotics in eldercare, MARL methodologies, and multi-robot coordination. Section \ref{sec:approach} delineates the simulation setup, MARL algorithm design, task allocation strategy, integration of dynamic obstacles, and simulation constraints. Section \ref{sec:methodology} provides a detailed description of the MARL agents' architecture and implementation timeline. Experimental results benchmarking COMA, FACMAC, and a MAPF baseline across critical metrics such as efficiency, collision frequency, and task success rates are presented in Section \ref{sec:implementation}. Section \ref{sec:discussion} discusses the limitations of our approach and implications of the findings. Finally, Section \ref{sec:conclusion} concludes with future research directions, including the potential for utilizing digital twin simulations and real-world deployment.

\section{Related Work}
\label{sec:related}
In recent years, the integration of multi-robot systems within healthcare settings has gained prominence, driven by the demand for innovative solutions to alleviate the burden on healthcare professionals. Service robots, spanning from logistic delivery carts to social companions, contribute significantly to this domain by enhancing operational efficiency and supporting elderly care \citep{bonaccosi2016}. For instance, pilot projects leveraging ROS (Robot Operating System) have demonstrated the effectiveness of drug delivery and home assistance robots within nursing homes, reducing the workload of staff while ensuring timely care \citep{salinas2023}.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{architecture_overview.png}
    \caption{Flow between webots , ROS 2 and MARL agents }
    \label{fig:placeholder}
\end{figure}
Nevertheless, the dynamics in healthcare environments present complex challenges, particularly when multiple robots operate concurrently. Conventional rule-based approaches or fixed scheduling systems often fall short in adapting to the fluid nature of these settings. Advances in multi-robot coordination strategies, such as the Contract Net Protocol and auction-based frameworks, have been employed to allocate tasks dynamically. These systems, while effective for initial task assignment, typically do not account for real-time changes in conditions or unforeseen obstacles \citep{kayy2017, salinas2023}. Research highlights that multi-agent pathfinding (MAPF) methods can optimize conflict-free navigation for multiple robots; however, traditional algorithms assume a static setting, which is seldom the case in dynamic care facilities populated by moving individuals \citep{sharon2015}.

To address these limitations, adaptive mechanisms, such as Multi-Agent Reinforcement Learning (MARL), have emerged as a preferred framework for managing robot behaviors in unpredictable environments. MARL enables agents to learn and recalibrate their strategies through interaction, thus facilitating improved navigation and collaboration among robots \citep{liu2023}. Various algorithms within this paradigm, such as COMA (Counterfactual Multi-Agent) and FACMAC (Factored Multi-Agent Centralized Policy Gradients), enhance decision-making processes by introducing a centralized critic that aids in effective credit assignment among agents \citep{wang2023a}. The application of these algorithms in the context of healthcare logistics presents a promising avenue, allowing for more responsive and efficient service delivery.

Moreover, the human-robot interaction aspect is crucial, especially given the proximity of nursing robots to elderly patients. Effective navigation must not only prevent physical collisions but also adhere to socially acceptable behaviors, such as yielding to human movement \citep{rostumi2019}. Recent efforts have incorporated simulations that model human behaviors within training frameworks, thus enhancing system adaptability when robots encounter real-world human dynamics. Aligning this approach with current trends in large language models (LLMs) facilitates a nuanced comprehension of human decision-making and interaction, thereby enhancing robot navigation policies \citep{yang2022}.

In conclusion, the synthesis of MARL with traditional planning approaches and human-in-the-loop simulations positions our research at the forefront of multi-robot healthcare systems. This integrated methodology not only exemplifies the potential of adaptive learning in chaotic environments but also underscores the ethical dimensions of deploying autonomous agents in sensitive settings such as eldercare.

\section{Approach and Model}
\label{sec:approach}
Our approach combines MARL-based navigation, auction-based task allocation, and realistic simulation of the environment. We first outline the problem model.

\subsection{Environment and Task Definition}
We considered a simulated urban neighborhood containing an elderly care facility and other buildings. The roads and walkways form a grid-like layout. Delivery tasks involve picking up medicine from a fixed location (e.g., a CBS-like pharmacy) and delivering it to the care facility or individual units. Robots start from a charging station or idle positions and await delivery requests. A request specifies a pickup point and a drop-off point; upon assignment, a robot must navigate to the pickup, then to the drop-off, avoiding obstacles and traffic.

The simulation includes:
\begin{itemize}
    \item \textbf{Static obstacles}: Buildings, benches, walls, etc., known from the map.
    \item \textbf{Dynamic obstacles}: Other robots, pedestrians, and occasional vehicles. Pedestrians follow sidewalks and crosswalks, sometimes jaywalking (with LLM-driven unpredictability) \citep{yang2022}. Vehicles follow scripted routes \citep{ztouni2021}.
    \item \textbf{Stochasticity}: Pedestrian space times, walking speeds, and vehicle traffic vary run-to-run.
\end{itemize}
Time is discretized into simulation steps (e.g., 100 ms per step). Each robot can execute a control action every step.

\subsection{Robot Model}
Each autonomous mobile robot is modeled as a differential-drive ground vehicle with a radius of 0.3 m. The robots are equipped with simulated sensors for navigation \citep{ztouni2021, galiat2023}. The action space is discrete: move forward, turn left, turn right, or stop, executed for 0.5 s per action.

\subsection{State Representation}
Each robot’s observation includes:
\begin{itemize}
    \item Simulated LiDAR scan (24 beams, normalized distances).
    \item Robot’s velocity (linear and angular).
    \item Egocentric direction to the goal (angle and distance).
    \item Binary flags for package carrying and pedestrian proximity.
\end{itemize}
The centralized critic (during training) accesses joint robot positions and goal locations. Robots do not directly observe other robots’ goals, learning coordination implicitly \citep{choi2023}.

\subsection{Communication}
ROS 2 topics handle task allocation and status sharing. The MARL policies execute without inter-robot communication, relying on observations. An auction node broadcasts tasks, robots bid based on travel time, and the lowest bid wins \citep{kayy2017}.

\subsection{Reward Design}
The reward function includes:
\begin{itemize}
    \item \textbf{Task Completion}: +100 for delivery completion (shared reward).
    \item \textbf{Time Penalty}: -1 per second per robot.
    \item \textbf{Collision Penalty}: -50 for collisions with robots, pedestrians, or vehicles.
    \item \textbf{Near-miss Penalty}: Small penalty for distances < 0.5 m to obstacles.
\end{itemize}
COMA’s counterfactual baseline and FACMAC’s factored critic handle credit assignment \citep{wang2023a}.

\section{Methodology}
\label{sec:methodology}
To effectively train and assess Multi-Agent Reinforcement Learning (MARL) algorithms for a medicine delivery task in a simulated elderly care facility, we have constructed a robust multi-layer control framework and a high-fidelity simulation environment. The architecture and methodologies utilized encompass several key components, highlighted in the following subsections.

\subsection{Webots Simulation Setup}
Utilizing the Webots simulation environment, we have developed a realistic indoor model simulating an assisted living facility. This environment includes accurately positioned patient rooms, storage spaces, and a nurse station organized within a grid layout \citep{ztouni2021}. With corridors designed to be 2 meters wide, the simulation allows both human and robot traffic interactions. The differential-drive robots, sized approximately 0.5 m x 0.5 m, represent small delivery robots, equipped with advanced sensing capabilities including a 2D LiDAR that detects obstacles up to 10 meters away, and wheel encoders for odometry \citep{ztouni2021}. The implementation of realistic sensor noise and latency within the communication network mimics real-world conditions effectively \citep{ztouni2021}.

\subsection{Design of the MARL Algorithm: COMA and FACMAC}
The MARL algorithms, namely COMA (Counterfactual Multi-Agent) and FACMAC (Factored Multi-Agent Centralized Policy Gradients), are pivotal in structuring the task as a cooperative multi-agent challenge. Each robot receives its observations, comprising its state, velocity, and local LiDAR data, which are essential for efficient navigation and decision-making \citep{zhang2024, park2022}. The state representation incorporates minimal communication regarding other agents’ positions to facilitate collaborative behavior without overwhelming the communication network \citep{choi2023}. Furthermore, reward structures have been designed to encourage timely task completions, efficient routes, collision avoidance, and proactive behaviors \citep{yu2023}.

To enhance training efficiency, policy gradients are calculated using centralized critics in both COMA and FACMAC frameworks, which improves the agents’ convergence capabilities during the training process, structured through approximately 10,000 episodes in a stochastic environment with varying task parameters \citep{salinas2023}.

\subsection{Auction-Based Task Allocation Mechanism}
Task allocation is executed through an auction mechanism, where each robot bids on tasks based on their current workload and travel estimates \citep{galiat2023}. This decentralized method enhances efficiency and speed, with the coordinator assigning tasks based on the lowest bid, thereby facilitating real-time task management and adaptation to changes in the environment \citep{kayy2017}. The integration of this auction system allows for quick reallocation of tasks in scenarios involving dynamic obstacles, making the approach resilient and operationally flexible \citep{gerkey2023}.

\subsection{Dynamic Obstacle Modeling with LLM-Driven Pedestrians}
To simulate a realistic operational environment, dynamic obstacles are introduced via LLM-generated behaviors for pedestrians and service carts \citep{choi2023}. This modeling represents the unpredictability inherent in real-world settings, where human behaviors are influenced by various factors \citep{yang2022}. The training incorporates three dynamic entities, requiring robots to adaptively learn avoidance strategies through reactive policies prompted by real-time sensor data \citep{salinas2023}. By accepting stochastic behavior patterns, our training aims to build robust coordination policies that function effectively in varied scenarios, tapping into the rich dynamics of human-robot interaction \citep{yang2022, rostumi2019}.

\subsection{Sensor and Simulation Constraints}
In modeling simulation constraints, various specifications have been drawn from real-world data to ensure that the policies learned in simulation can be effectively translated to physical robots \citep{yang2022}. Incorporating communication delays and reliable assessment in our methodology allows MARL systems to learn within conditions of partial observability and latency \citep{liu2023}. By deploying ROS 2 alongside Webots, we ensure seamless integration of software components required for the simulation, paving the way for practical implementation \citep{ztouni2021}.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{Robot Navigation in Sim Environment}
    \label{fig:placeholder}
\end{figure}
\section{Implementation}
\label{sec:implementation}
This section outlines the systematic process through which our Multi-Agent Reinforcement Learning (MARL) methodology was implemented over a nine-week development period. Each week was characterized by specific deliverables, from initial literature reviews to final testing, culminating in a comprehensive evaluation of our approach to robot coordination within an eldercare environment.

\subsection{Project Timeline}
Table \ref{tab:timeline} summarizes the milestones achieved throughout the project, highlighting pivotal activities undertaken each week. The tasks followed a logical progression, beginning with foundational research and culminating in practical implementation and evaluation of the developed MARL algorithms.

\begin{table}[h]
\caption{Project Timeline and Milestones}
\label{tab:timeline}
\begin{tabular}{|c|p{6cm}|}
\hline
\textbf{Week} & \textbf{Deliverables/Milestones} \\
\hline
1 & Reviewed MARL algorithms (COMA, FACMAC) and eldercare robotics literature to define the problem scope \citep{wang2023a, salinas2023}. \\
2 & Built Webots simulation of an eldercare facility with static obstacles, validated robot movement \citep{ztouni2021}. \\
3--4 & Designed and implemented MARL algorithms (COMA, FACMAC) \citep{wang2023a}. \\
5--6 & Integrated task allocation and dynamic obstacle modeling \citep{kayy2017, yang2022}. \\
7--8 & Trained and evaluated MARL policies \citep{salinas2023}. \\
9 & Analyzed results and documented findings. \\
\hline
\end{tabular}
\end{table}

\subsection{COMA Agent Architecture}
The architecture of the MARL agents, comprised of both COMA and FACMAC implementations, aligns with established neural network structures tailored for actor and critic roles. Each robot’s actor, defined as a deep neural network, produces action probabilities based on its observations using a softmax function. A simpler, stacked connected network framework (two layers—128 and 64 neurons with ReLU activations) was employed \citep{salinas2023}. The integration of the learning module with the Webots environment was facilitated via a ROS 2 bridge, ensuring real-time synchronization between agent actions and environmental states \citep{ztouni2021}. Our training cycles, executed in accelerated simulations, enabled rapid evolution through multiple episodes, quickly reverting to initial states after each episode. Moreover, evaluation in real-time involved deploying learned policies as ROS 2 nodes, allowing a seamless transition from training to functional deployment \citep{ztouni2021}.

The COMA code implementation comprises approximately 500 lines of Python code, meticulously annotated and structured to reflect the underlying theoretical models. A detailed code section included in the Appendix provides an illustrative sample of the training loop, delineating critical processes like the computation of the advantage function and actor network updates.

\subsection{Experimental Setup}
Our evaluation prominently utilized scenarios encompassing two and three robot formations within the Webots simulation. Delivery tasks were randomly generated, with parameters such as target rooms and spawn timings established within a five-minute window, complemented by the introduction of dynamic obstacles including a pedestrian and a moving cart.





\textbf{Three primary strategies were scrutinized:}
\begin{enumerate}
    \item \textbf{Centralized MAPF Baseline}: An offline planner governed task assignments through single auctions, neglecting dynamic environmental factors. Robots were permitted to move only upon persistent observation policy over 5 seconds \citep{yang2022}.
    \item \textbf{COMA MARL Policy}: Implemented individual task assignments via auctioning while relying upon the derived policies absent of centralized planning \citep{wang2023a}.
    \item \textbf{FACMAC MARL Policy}: Similar to COMA, with integrated attributes therein \citep{wang2023a}.
\end{enumerate}

Metrics assessed included frequency of collisions, average delivery times, completion rates, and throughput, encompassing qualitative observations of cooperative maneuvers within the robotic trajectories \citep{salinas2023}.

\subsection{Task Performance}
Our findings unequivocally indicated MARL-based strategies markedly surpassed the MAPF baseline in task completion rates, particularly under dynamic conditions, as delineated in Table \ref{tab:comparison}. Notably, MAPF achieved an average on-time completion rate of 80\%, while MARL strategies (COMA and FACMAC) achieved 92--95\% completion rates. The average delivery durations further corroborate this inference, with MARL agents completing tasks remarkably faster (COMA: 45 seconds, FACMAC: 42 seconds) compared to MAPF’s 60 seconds \citep{salinas2023}.

\subsection{Collision Avoidance}
Collision metrics elucidated profound differences in navigational proficiency: both COMA and FACMAC adeptly mitigated collision incidences through thoughtful reward structures oriented towards safety. Over the testing period, COMA and FACMAC reported four and two collisions, respectively, against MAPF’s considerable tally of fifteen, demonstrating the resilience of MARL protocols in avoiding critical interactions in precarious situations \citep{salinas2023}.

\subsection{Throughput}
Throughput metrics revealed deeper insights into operational efficiencies. MAPF’s rigidity led to lower task completion rates, while COMA and FACMAC utilized adaptive navigation strategies to achieve higher throughputs (71.5 and 73.2 tasks per hour, respectively, vs MAPF’s 65.1) \citep{salinas2023}. These results reflect sharper navigational strategies that minimized delays and optimized task assignments \citep{gerkey2023}.

\subsection{Qualitative Observations}
Qualitatively, the behaviors exhibited by the MARL agents presented intriguing adaptive strategies, such as deferred movements allowing collaborative passage in confined spaces. Observations of pedestrian interactions illustrated the agents’ ability to navigate around dynamic obstacles effectively \citep{rostumi2019}.



\begin{table}[h]
\caption{Comparison of MARL (COMA) vs. Centralized MAPF Baseline}
\label{tab:comparison}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{MARL (COMA)} & \textbf{MAPF Baseline} \\
\hline
Delivery Time (s) & 45.0 & 60.0 \\
Collisions/10 tasks & 4 & 15 \\
Throughput (tasks/h) & 71.5 & 65.1 \\
\hline
\end{tabular}
\end{table}

\section{Discussion}
\label{sec:discussion}
While our simulation demonstrated proficiency with three to four robots, deploying ten robots in a larger facility necessitates considering more advanced MARL algorithms or decentralized approaches for coordination. The absence of explicit communication among our MARL agents emerged as a critical limitation. Literature suggests that establishing communication protocols can enhance the efficacy of multi-robot systems, particularly in contexts requiring robust coordination \citep{ztouni2021, ivanovic2019}. Although our focus was on testing implicit coordination strategies, findings indicate that explicit communication among robots may be essential to reduce conflicts and optimize path planning as team sizes grow \citep{liu2023}.

Moreover, conducting our experiments in simulated environments raises concerns about real-world applicability, particularly with unpredictable human behaviors \citep{liu2023, yang2022}. Previous works have highlighted the necessity of integrating human-aware navigation strategies into robotic systems to enhance social compliance and safety \citep{rostumi2019, yang2022}. Human behaviors such as sudden movements can significantly impact robotic operations; thus, incorporating social norms into our reward structure could help mitigate potential disruption during real-world deployments \citep{yang2022, rostumi2019}. Our findings suggest that further research into adaptive policies that incorporate these social dynamics is essential for advancing practical implementations of MARL systems in human-populated environments.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{collision_stats (1).png}
    \caption{Collision per episode}
    \label{fig:placeholder}
\end{figure}
In conclusion, while MARL presents substantial opportunities to enhance the coordination of autonomous robots in complex environments, the integration of traditional methods (such as task allocation) and realistic modeling (including sensor limitations and human behavior) remains crucial. The lessons learned from our work have potential applications in various robotic domains, such as hospital supply delivery and office automation. We believe our framework can significantly contribute to developing more efficient and safe robotic systems in healthcare and beyond \citep{ivanovic2019, schreiders2021}. Moving this research toward real-world testing in senior living facilities, supported by a digital twin approach, will be our primary focus in future work \citep{yoo2023, yoo2023a}.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{energy_usage.png}
    \caption{Energy Consumption per Episode }
    \label{fig:placeholder}
\end{figure}
Contributions: For a safety-critical application, this work offers an integrated framework that connects robotic systems and reinforcement learning. We demonstrated that MARL can manage complex coordination (such as negotiating hallway rights-of-way) in a decentralized way by utilizing centralized critics and forming rewards \citep{wang2023a}. An innovative feature that increased the realism of our tests was the addition of a LangChain-based pedestrian simulation; this could be a helpful resource for others looking to incorporate human behavior into training simulations \citep{yang2022, rostumi2019}. Furthermore, my method combines the advantages of machine learning and operations research by employing MARL for path execution and auctions for task assignment, demonstrating the efficacy of a hierarchical approach \citep{kayy2017, gerkey2023}. The outcome is a reliable multi-robot system design that is prepared for practical testing in senior living facilities.


a)The trajectory plot showing robot and pedestrian movements in the simulated care facility: 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{The trajectory plot showing robot and pedestrian movements in the simulated care facility }
    \label{fig:placeholder}
\end{figure}
\section{Conclusion}
\label{sec:conclusion}
In this paper, we used adaptive MARL techniques to present a thorough simulation-based study of coordinating multiple autonomous robots for medicine delivery in an elderly care facility. We developed a rich testbed for multi-robot coordination by combining two state-of-the-art multi-agent reinforcement learning algorithms (COMA and FACMAC) with traditional task allocation and a novel LLM-driven environment. Our findings showed that by attaining greater task success rates, almost zero collisions, and enhanced energy efficiency, learning-based strategies can perform better than traditional planning in dynamic, unpredictable environments. The learned policies were kept grounded and practical for use in the real world by incorporating hardware limitations and sensor noise during training.

\subsection{Future Work}
We intend to move this work from simulation to real-world robots. In a lab environment that simulates a hospital hallway, we will first apply the learned policies to two actual mobile robots (fitted with LiDAR and ROS 2) \citep{ztouni2021}. We will employ a digital twin strategy, in which sensor data is used to keep the simulation environment in sync with the real world \citep{yoo2023, yoo2023a}. This will enable us to track the behavior of the policy and take appropriate action (for safety) while progressively boosting confidence. Through methods like domain randomization or continuous learning (perhaps by using reinforcement learning directly on the robot in brief trials), we expect to need to adjust the policies with real-world data \citep{yang2022}. Adaptive scheduling or planning is another extension. For example, scheduling decisions can be informed by the learned value function from our critic (the critic’s estimate of future reward could indicate whether a robot is likely to be delayed, etc.), which could feed into a higher-level scheduler. Furthermore, we are interested in investigating communication between robots. At the moment, all coordination is implicit, but efficiency may be further increased, particularly in larger teams, by providing robots with a learned communication protocol (using techniques like CommNet or MODDPO) \citep{ivanovic2019, yang2022}.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{success_rate.png}
    \caption{Task Success Rate}
    \label{fig:placeholder}
\end{figure}
In terms of algorithms, we want to look into different MARL algorithms (like QMIX and MADDPG variations) and even employ curriculum learning to see if training can be made faster or better \citep{wang2023a, yang2022}. To help scale to larger problems, one approach is to train on simpler scenarios (e.g., with fewer obstacles) and then gradually increase the difficulty (e.g., more people, more robots). Using a multi-objective RL approach or explicitly including social norms or comfort distances in the reward could be beneficial for the human-robot interaction component \citep{yang2022, rostumi2019}. We could use social navigation research to penalize actions that, although efficient, humans may interpret as aggressive (e.g., squeezing through a gap directly in front of a walking person might scare them, even if no collision occurs). Real deployment in senior care, where resident and staff acceptance is critical, will require striking a balance between efficiency and such social compliance \citep{ivanovic2019, rostumi2019}.

Lastly, we intend to make our learned models and simulation framework publicly available (the Appendix contains links to the GitHub repository). This will enable other researchers to build upon our system and replicate our findings. We intend to provide a practical digital twin environment for researching multi-robot coordination in human-populated areas by sharing the Webots world, ROS 2 nodes, and training code \citep{ztouni2021, salinas2023}. In a larger sense, our work is a step toward more intelligent healthcare facilities that integrate humans and robots. Such systems could significantly enhance healthcare delivery, increase safety (by lowering human error in medication distribution), and lessen the workload for care staff as MARL and robotics technology advance.

\bibliographystyle{plainnat}


\begin{thebibliography}{1}

\bibitem{bao2021}
L. Bao, Y. Li, and Z. Wang, ``Robotic assistance for eldercare: Automating medication delivery in assisted living facilities,'' \emph{Journal of Medicare Robotics}, vol. 3, no. 2, pp. 89--102, 2021.

\bibitem{jeon2016}
S. Jeon and J. Lee, ``Multi-robot multi-task allocation for hospital logistics,'' in \emph{Proc. Int. Conf. Advanced Communications and Technology (ICACT)}, 2016, pp. 89--94.

\bibitem{song2020}
H. Song and B. Sun, ``Dynamic obstacle avoidance for autonomous robots in eldercare settings,'' \emph{Journal of Automated Robotics Research}, vol. 30, no. 3, pp. 365--382, 2020.

\bibitem{wang2023}
L. Wang, J. Zhang, and K. Li, ``Multi-agent path finding for autonomous delivery robots in dynamic environments,'' \emph{J. Medicare Intelligence Review}, vol. 30, no. 3, pp. 201--215, 2023.

\bibitem{sharon2015}
G. Sharon, R. Stern, A. Felner, and N. Sturtevant, ``Conflict-based search for optimal multi-agent pathfinding,'' \emph{Artificial Intelligence}, vol. 219, pp. 40--66, 2015.

\bibitem{liu2023}
X. Liu, Y. Wu, and J. Chen, ``Multi-agent reinforcement learning for real-time coordination in dynamic environments,'' \emph{IEEE Transactions on Intelligent Systems}, vol. 25, no. 3, pp. 1254--1265, 2023.

\bibitem{yu2023}
C. Yu and H. Zhang, ``MARL applications in traffic and warehouse automation: A survey,'' \emph{IEEE Transactions on Intelligent Transportation Systems}, vol. 25, no. 3, pp. 1254--1265, 2023.

\bibitem{salinas2023}
J. Salinas-Avila, R. F. Corosado, and J. L. Gordillo, ``A ROS-based autonomous medication delivery robot for nursing homes,'' in \emph{Proc. IEEE Int. Conf. Automated Sci. Eng.}, vol. 3, no. 5, 2023.

\bibitem{kayy2017}
O. Kayy, ``Auction-based task allocation for multi-robot coordination,'' \emph{Journal of Int. Conf. Robotics Research}, vol. 36, no. 4, pp. 456--470, 2017.

\bibitem{rostumi2019}
A. Rostumi, H. Pandey, and V. Napp, ``LLM-driven pedestrian simulation for autonomous robot navigation,'' in \emph{Proc. IEEE Int. Conf. Simulation Modeling and Programming for Autonomous Robots (SIMPAR)}, 2019, pp. 125--150.

\bibitem{bonaccosi2016}
M. Bonaccosi, L. Fiorini, F. Cavallo, A. Saffiotti, and P. Dario, ``A cloud robotics solution to improve social and future robotics for health and service,'' in \emph{Proc. Int. Conf. Advanced Robotics (ICAR)}, 2016, pp. 145--150.

\bibitem{gerkey2023}
B. P. Gerkey and M. J. Mataric, ``Sold! Auction methods for multi-robot coordination,'' \emph{IEEE Transactions on Robotics and Automation}, vol. 18, no. 5, pp. 758--768, 2023.

\bibitem{wang2023a}
J. Wang, C. S. Wang, and S. Wang, ``Counterfactual multi-agent policy gradients,'' in \emph{Proc. AAAI Conf. Artificial Intelligence}, 2019, pp. 2544--2555.

\bibitem{yang2022}
H. Yang, ``LLM-driven human behavior modeling for robot navigation,'' in \emph{Proc. IEEE Int. Conf. Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)}, 2022, pp. 89--95.

\bibitem{ztouni2021}
F. Ztouni, S. Mbalaj, and M. Ati, ``Webots-based simulation for autonomous navigation in elderly care facilities,'' in \emph{Proc. Int. Conf. Robotics and Automation (ICRA)}, 2021, pp. 234--240.

\bibitem{galiat2023}
K. Galiat, M. Coti, and F. Aungge, ``Sensor-based navigation for autonomous mobile robots in dynamic environments,'' \emph{Journal of Field Robotics}, vol. 40, no. 2, pp. 234--250, 2023.

\bibitem{zhang2024}
H. L. J. Zhang, and H. Wang, ``MARL-based navigation for autonomous robots in complex environments,'' \emph{IEEE Transactions on Robotics}, vol. 39, no. 3, pp. 789--803, 2024.

\bibitem{park2022}
F. Park, S. Kim, and Y. Lee, ``State representations for intelligent reinforcement learning in robotic systems,'' in \emph{Proc. IEEE Int. Conf. Intelligent Robots and Systems (IROS)}, 2022, pp. 89--95.

\bibitem{choi2023}
A. Y. Choi, and S. Khan, ``Decentralized MARL with minimal communication for robotic coordination,'' \emph{Robotics and Autonomous Systems}, vol. 61, pp. 89--94, 2023.

\bibitem{ivanovic2019}
B. Ivanovic, E. Schmerling, and M. Pavone, ``Multi-robot coordination for healthcare logistics: A reinforcement learning approach,'' in \emph{Proc. Int. Conf. Intelligent Robots and Systems (IROS)}, 2019, pp. 2435--2451.

\bibitem{schreiders2021}
E. Schreiders, M. Luck, and J. Pitt, ``Multi-robot systems for hospital logistics: Opportunities and challenges,'' \emph{Journal of Autonomous Robots}, vol. 43, no. 5, pp. 1244--1245, 2021.

\bibitem{yoo2023}
J. Yoo, J. Wang, and Z. Li, ``Digital twin approaches for multi-robot coordination in healthcare settings,'' \emph{IEEE Transactions on Autonomous Science and Engineering}, vol. 20, no. 5, pp. 1--10, 2023.

\bibitem{yoo2023a}




J. Yoo, J. Wang, and Z. Li, ``Digital twin approaches for multi-robot coordination in healthcare settings,'' \emph{IEEE Transactions on Autonomous Science and Engineering}, vol. 20, no. 5, pp. 1--10, 2023.
\bibitem{}
OpenAI. (2023). OpenAI models. https://www.openai.com



\appendices
\section{1.\textbf{COMA Training Loop}} 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Screenshot 2025-08-03 104105.png}
    \caption{COMA TRAINING LOOP
    2.}
    \label{fig:placeholder}
\end{figure}


 


GitHub /repository: {https://github.com/pokhrelsushil}
{https://github.com/pokhrelsushil/Coordination-for-Urban-Medicine-Delivery-in-Elderly-Care-Facilities

\bigskip
\noindent\textbf{Note:} This paper is intended for submission to arXiv after its course project submission.




\end{thebibliography}

\end{document}\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{1}
\author{Sushil Pokhrel}
\date{August 2025}

\begin{document}

\maketitle

\section{Introduction}

\end{document}

% ===== Preamble Packages (Add in your document preamble) =====
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}

% Optional: Customize Python syntax highlighting
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% ===== Main Document Content =====
\appendices
\section{COMA Training Loop}
Below is a sample implementation of the COMA training loop, adapted from the described architecture~\cite{arce2023comparison}.

\begin{lstlisting}[language=Python, caption=Sample COMA Training Loop, label={lst:coma-training}]
import torch
import torch.nn as nn
import numpy as np

class COMAAgent(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(COMAAgent, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(input_dim * n_agents, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, obs):
        return self.actor(obs)

def train_coma(agents, env, episodes=1000):
    optimizer = torch.optim.Adam(agents.parameters())
    for episode in range(episodes):
        obs = env.reset()
        done = False
        episode_rewards = []
        while not done:
            actions = []
            for agent, ob in zip(agents, obs):
                action_probs = agent(ob)
                action = torch.multinomial(action_probs, 1)
                actions.append(action)
            next_obs, rewards, done, _ = env.step(actions)
            episode_rewards.append(rewards)
            obs = next_obs
        # Compute advantage and update
        advantages = compute_advantage(agents, episode_rewards)
        loss = compute_coma_loss(agents, advantages)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Placeholder for advantage and loss computation
def compute_advantage(agents, rewards):
    return np.array(rewards)  # Simplified for illustration

def compute_coma_loss(agents, advantages):
    return torch.tensor(0.0)  # Simplified for illustration
\end{lstlisting}

\bibliographystyle{plainnat}
\bibliography{references}
% ===== Preamble Packages (Add in your document preamble) =====
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}

% Optional: Customize Python syntax highlighting
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% ===== Main Document Content =====
\appendices
\section{COMA Training Loop}
Below is a sample implementation of the COMA training loop, adapted from the described architecture~\cite{arce2023comparison}.

\begin{lstlisting}[language=Python, caption=Sample COMA Training Loop, label={lst:coma-training}]
import torch
import torch.nn as nn
import numpy as np

class COMAAgent(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(COMAAgent, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(input_dim * n_agents, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, obs):
        return self.actor(obs)

def train_coma(agents, env, episodes=1000):
    optimizer = torch.optim.Adam(agents.parameters())
    for episode in range(episodes):
        obs = env.reset()
        done = False
        episode_rewards = []
        while not done:
            actions = []
            for agent, ob in zip(agents, obs):
                action_probs = agent(ob)
                action = torch.multinomial(action_probs, 1)
                actions.append(action)
            next_obs, rewards, done, _ = env.step(actions)
            episode_rewards.append(rewards)
            obs = next_obs
        # Compute advantage and update
        advantages = compute_advantage(agents, episode_rewards)
        loss = compute_coma_loss(agents, advantages)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Placeholder for advantage and loss computation
def compute_advantage(agents, rewards):
    return np.array(rewards)  # Simplified for illustration

def compute_coma_loss(agents, advantages):
    return torch.tensor(0.0)  # Simplified for illustration
\end{lstlisting}

\bibliographystyle{plainnat}
\bibliography{references}
\href{}{}
\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    frame=single,
    breaklines=true,
    breakatwhitespace=true
}

\begin{document}

\appendices
\section{COMA Training Loop}
The following is a sample implementation of the COMA training loop, adapted from the described architecture \cite{arce2023comparison}.

\begin{lstlisting}
import torch
import torch.nn as nn
import numpy as np

class COMAAgent(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(COMAAgent, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(input_dim * n_agents, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, obs):
        return self.actor(obs)

def train_coma(agents, env, episodes=1000):
    optimizer = torch.optim.Adam(agents.parameters())
    for episode in range(episodes):
        obs = env.reset()
        done = False
        episode_rewards = []
        while not done:
            actions = []
            for agent, ob in zip(agents, obs):
                action_probs = agent(ob)
                action = torch.multinomial(action_probs, 1)
                actions.append(action)
            next_obs, rewards, done, _ = env.step(actions)
            episode_rewards.append(rewards)
            obs = next_obs
        # Compute advantage and update
        advantages = compute_advantage(agents, episode_rewards)
        loss = compute_coma_loss(agents, advantages)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Placeholder for advantage and loss computation
def compute_advantage(agents, rewards):
    return np.array(rewards)  # Simplified for illustration

def compute_coma_loss(agents, advantages):
    return torch.tensor(0.0)  # Simplified for illustration
\end{lstlisting}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}